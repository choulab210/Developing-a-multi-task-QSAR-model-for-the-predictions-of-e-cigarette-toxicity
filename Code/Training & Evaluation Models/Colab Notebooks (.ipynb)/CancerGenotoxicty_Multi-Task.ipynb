{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOexvdYn+yIU0xtWfmfJUQJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# %% ============================================================================\n","# Multi-Task DNN-QSAR Models for simultaneous prediction of Carcinogenicity and Genotoxicity\n","# Author: Alexa Canchola, Kunpeng Chen\n","# Advisor: Wei-Chun Chou\n","# Date: July 14, 2025\n","# ==============================================================================\n","\"\"\"\n","This script builds and evaluates a multi-task deep neural network QSAR (DNN-QSAR) model for the simultaneous prediction of:\n"," - Carcinogenicity\n"," - Genotoxicity\n","\n","The model is developed for organic compounds associated with e-cigarettes.\n","\"\"\""],"metadata":{"id":"fQY623mU7UZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWQyzwYlIT47","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757955100590,"user_tz":420,"elapsed":24264,"user":{"displayName":"Alexa Canchola","userId":"12486619005608823341"}},"outputId":"763cf5d6-b452-4028-ef14-5046bd94db34"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# ============================================================================\n","# Install Required Dependencies\n","# ============================================================================\n","!pip install -q rdkit shap scikit-optimize torch torchvision -U"]},{"cell_type":"code","source":["# ============================================================================\n","# Import Required Libraries\n","# ============================================================================\n","# Standard Libraries for Data Handling  & Visualization\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import random, warnings, time\n","\n","\n","# Import Required PyTorch Libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Scikit-learn (sklearn): Model Selection & Evaluation  performance\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import (accuracy_score, roc_auc_score, balanced_accuracy_score, roc_curve, auc, confusion_matrix,\n","                             matthews_corrcoef, precision_score, recall_score, f1_score)\n","from sklearn.exceptions import UndefinedMetricWarning\n","\n","# Bayesian Optimization (for hyperparameter tuning)\n","from skopt import gp_minimize\n","\n","# RDKit: Molecular Fingerprint & Descriptor Calculations\n","from rdkit import Chem, DataStructs\n","from rdkit.Chem import AllChem, MACCSkeys, RDKFingerprint, Descriptors\n","from rdkit.Chem.AllChem import GetMorganGenerator\n","from rdkit.DataStructs import ConvertToNumpyArray\n","from rdkit.ML.Descriptors import MoleculeDescriptors\n","from rdkit import RDLogger\n","\n","# Feature Scaling for RDKit Descriptors\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Applicaibility Domain Calculation\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.decomposition import PCA\n","\n","#Feature Importance analysis\n","import shap\n","\n","#For usage in Google Colab\n","from google.colab import files\n","import io\n","\n","import joblib\n","import os"],"metadata":{"id":"PRHXzrOSIgei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Filter Extraneous Warnings\n","# ============================================================================\n","warnings.filterwarnings('ignore', category=UserWarning)\n","warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n","warnings.filterwarnings('ignore', category=RuntimeWarning)\n","# Disable ALL RDKit logs\n","RDLogger.DisableLog('rdApp.*')"],"metadata":{"id":"Gomsxl60Ikxr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %% ============================================================================\n","# Import Data\n","# ============================================================================\n","# Read raw data in Python IDE other than Google Colab\n","# dataset_raw = pd.read_csv('Data_corrected.csv') # Replace with the correct path if needed\n","\n","# Load Data in Google Colab\n","ecig = files.upload() # Data_corrected.csv\n","ecig_file = list(ecig.keys())[0]\n","dataset_raw = pd.read_csv(io.BytesIO(ecig[ecig_file]))"],"metadata":{"id":"hApnAn2fOVTi","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1757955258806,"user_tz":420,"elapsed":136652,"user":{"displayName":"Alexa Canchola","userId":"12486619005608823341"}},"outputId":"96622e43-27a5-4730-8b66-59f4586e990d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-dc0b0ea4-bd21-4804-9c1e-3a751831c264\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-dc0b0ea4-bd21-4804-9c1e-3a751831c264\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Data_corrected.csv to Data_corrected.csv\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# Preprocess Data\n","# ============================================================================\n","column_names = ['Chemicals',\n","                'CAS',\n","                'SMILES',\n","                'Cancer',\n","                'Genotoxicity',\n","]\n","\n","remove_elements = [[],\n","                   [],\n","                   [],\n","                   [],\n","                   [],\n","                   [],\n","                   [],\n","                   [],\n","                   []]\n","\n","def RemoveElements(df, column_names, remove_elements):\n","    for i in range(0,len(column_names)):\n","        for j in range(0,len(remove_elements[i])):\n","            df = df[df[column_names[i]] != remove_elements[i][j]]\n","\n","    return df\n","\n","data = RemoveElements(dataset_raw, column_names, remove_elements)\n","\n","#Check Data\n","print('Total number of data points: ' + str(len(data.index)))\n","print('--------------------------------------------------------------')\n","print(data.head())"],"metadata":{"id":"40tvWUVxIqfM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757955279655,"user_tz":420,"elapsed":33,"user":{"displayName":"Alexa Canchola","userId":"12486619005608823341"}},"outputId":"46d94140-9a4b-4057-bb8f-9e5cc803c534"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of data points: 966\n","--------------------------------------------------------------\n","                      Chemical_Names        CAS                       SMILES  \\\n","0                     Benzyl acetate   140-11-4          CC(=O)OCC1=CC=CC=C1   \n","1                       Benzophenone   119-61-9  O=C(C1=CC=CC=C1)C1=CC=CC=C1   \n","2                            Acetoin   513-86-0                  CC(O)C(C)=O   \n","3                       alpha-Ionone   127-41-3  CC(=O)\\C=C\\C1C(C)=CCCC1(C)C   \n","4  1,3-Dioxolane, 4-methyl-2-pentyl-  1599-49-1              CCCCCC1OCC(C)O1   \n","\n","  Cancer Genotoxicity Skin Eye Skin (Irritating) Skin (sensitizing)  \n","0      0            1    1   1                 1                  0  \n","1      1            1    0   1                 1                  0  \n","2     na            1    1   1                 1                 na  \n","3     na            1    0   0                 0                  0  \n","4     na           na   na  na                na                 na  \n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# Check Task Dsitributions\n","# ============================================================================\n","def CheckData(df, column_name):\n","    # Count occurrences of each task\n","    item_counts = df[column_name].value_counts()\n","\n","    # Print unique task counts\n","    print(\"Item  Count\")\n","    for item, count in item_counts.items():\n","        print(f\"{item}  {count}\")\n","\n","CheckData(data, 'Cancer') #balance\n","\n","CheckData(data, 'Genotoxicity')"],"metadata":{"id":"HrmVB2pxI1Dk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757955279724,"user_tz":420,"elapsed":66,"user":{"displayName":"Alexa Canchola","userId":"12486619005608823341"}},"outputId":"a23a2c71-cd24-4c5f-9544-f20790105838"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Item  Count\n","na  859\n","1  64\n","0  43\n","Item  Count\n","na  514\n","1  227\n","0  225\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# Define Functions for Model Evaluation & Metrics\n","# ============================================================================\n","def weighted_average(values, weights):\n","    # Ensure the lengths of roc_auc_each_output and weights are the same\n","    if len(values) != len(weights):\n","        raise ValueError(\"Length of values must be equal to the length of weights\")\n","\n","    # Calculate the weighted average\n","    weighted_sum = sum(value * weight for value, weight in zip(values, weights))\n","    total_weight = sum(weights)\n","\n","    # Return the weighted average\n","    return weighted_sum / total_weight if total_weight != 0 else 0\n","\n","def model_metrics(y_true, y_pred, y_pred_prob):\n","\n","    if (torch.is_tensor(y_true)):\n","        y_true = y_true.ravel().tolist()\n","    if (torch.is_tensor(y_pred)):\n","        y_pred = y_pred.ravel().tolist()\n","    if (torch.is_tensor(y_pred_prob)):\n","        y_pred_prob = y_pred_prob.ravel().tolist()\n","\n","    roc_auc = roc_auc_score(y_true, y_pred_prob)\n","    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n","    mcc = matthews_corrcoef(y_true, y_pred)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","\n","    return [roc_auc, balanced_accuracy, mcc, accuracy, precision, recall, f1]\n","\n","def calculate_cv_avg_std(outer_results):\n","\n","    # Calculate mean and std of all metrics from outer results\n","    avg_ncv_auc = np.mean([result[\"outer_auc\"] for result in outer_results])\n","    avg_ncv_balanced_acc = np.mean([result[\"outer_balanced_accuracy\"] for result in outer_results])\n","    avg_ncv_mcc = np.mean([result[\"outer_mcc\"] for result in outer_results])\n","    avg_ncv_accuracy = np.mean([result[\"outer_accuracy\"] for result in outer_results])\n","    avg_ncv_precision = np.mean([result[\"outer_precision\"] for result in outer_results])\n","    avg_ncv_recall = np.mean([result[\"outer_recall\"] for result in outer_results])\n","    avg_ncv_f1 = np.mean([result[\"outer_f1\"] for result in outer_results])\n","\n","    std_ncv_auc = np.std([result[\"outer_auc\"] for result in outer_results])\n","    std_ncv_balanced_acc = np.std([result[\"outer_balanced_accuracy\"] for result in outer_results])\n","    std_ncv_mcc = np.std([result[\"outer_mcc\"] for result in outer_results])\n","    std_ncv_accuracy = np.std([result[\"outer_accuracy\"] for result in outer_results])\n","    std_ncv_precision = np.std([result[\"outer_precision\"] for result in outer_results])\n","    std_ncv_recall = np.std([result[\"outer_recall\"] for result in outer_results])\n","    std_ncv_f1 = np.std([result[\"outer_f1\"] for result in outer_results])\n","\n","    # Print the summary of nested cross-validation results\n","    print('----------------------------------------------------------')\n","    print(\"Cross validation (mean ± std):\")\n","    print(\"AUC, balanced accuracy, MCC, accuracy, precision, recall, F1\")\n","\n","    print(f\"{avg_ncv_auc:.3f} ± {std_ncv_auc:.3f}, {avg_ncv_balanced_acc:.3f} ± {std_ncv_balanced_acc:.3f}, {avg_ncv_mcc:.3f} ± {std_ncv_mcc:.3f}, \"\n","          f\"{avg_ncv_accuracy:.3f} ± {std_ncv_accuracy:.3f}, {avg_ncv_precision:.3f} ± {std_ncv_precision:.3f}, \"\n","          f\"{avg_ncv_recall:.3f} ± {std_ncv_recall:.3f}, {avg_ncv_f1:.3f} ± {std_ncv_f1:.3f}\")\n","\n","    return avg_ncv_auc, avg_ncv_f1, avg_ncv_balanced_acc, avg_ncv_mcc, std_ncv_mcc"],"metadata":{"id":"jFQydciyKO8u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#%% ============================================================================\n","# Define Molecular Fingerprint Generation Function\n","# ============================================================================\n","\n","def smiles_to_feature(smiles, mode=\"MACCS\", **kwargs):\n","    \"\"\"\n","    Convert a SMILES string into a molecular feature representation.\n","\n","    Parameters\n","    ----------\n","    smiles : str\n","        The SMILES string representing the molecule.\n","    mode : str, default=\"MACCS\"\n","        The type of molecular representation to compute. Options are:\n","\n","        - \"MACCS\": MACCS structural keys (167-bit fingerprint).\n","\n","        - \"Morgan\": Morgan fingerprints Radius=3 and fpsize = 2048\n","\n","        - \"FCFP\": Feature-Class Fingerprints (FCFP)\n","\n","        - \"RDK\": RDKit topological/path-based fingerprint. fpsize=2048).\n","\n","        - \"Descriptors\": A vector of molecular descriptors from RDKit’s\n","          `Descriptors._descList` (e.g., molecular weight, logP, etc.).\n","\n","    **kwargs :\n","        Extra arguments passed depending on `mode`.\n","        - Morgan/FCFP: `radius` (int), `fpSize` (int), `include_chirality` (bool)\n","        - RDK: `fpSize` (int)\n","\n","    Returns\n","    -------\n","    np.ndarray\n","        A NumPy array representing the fingerprint or descriptor vector.\n","        The size depends on `mode`:\n","        - MACCS: 167\n","        - Morgan/FCFP/RDK: `fpSize` (default 2048)\n","        - Descriptors: number of available RDKit descriptors\n","\n","    Raises\n","    ------\n","    ValueError\n","        If the mode is unknown or unsupported.\n","\n","    \"\"\"\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None:\n","        if mode == \"MACCS\":\n","            return np.zeros(167, dtype=int)\n","        elif mode in [\"Morgan\", \"FCFP\", \"RDK\"]:\n","            fpSize = kwargs.get(\"fpSize\", 2048)\n","            return np.zeros(fpSize, dtype=int)\n","        elif mode == \"Descriptors\":\n","            desc_list = [desc[0] for desc in Descriptors._descList]\n","            return np.zeros(len(desc_list))\n","        else:\n","            raise ValueError(f\"Unknown mode: {mode}\")\n","\n","    if mode == \"MACCS\":\n","        fp = MACCSkeys.GenMACCSKeys(mol)\n","        arr = np.zeros((167,), dtype=int)\n","        ConvertToNumpyArray(fp, arr)\n","        return arr\n","\n","    elif mode == \"Morgan\":\n","        radius = kwargs.get(\"radius\", 3)\n","        fpSize = kwargs.get(\"fpSize\", 2048)\n","        generator = AllChem.GetMorganGenerator(radius=radius, fpSize=fpSize)\n","        fp = generator.GetFingerprint(mol)\n","        return np.array(fp)\n","\n","    elif mode == \"FCFP\": #Note: This method uses a deprecated version to generate FCFP that will be removed in future versions of RDKit\n","        radius = kwargs.get(\"radius\", 3)\n","        fpSize = kwargs.get(\"fpSize\", 2048)\n","        include_chirality = kwargs.get(\"include_chirality\", False)\n","        invariants = AllChem.GetFeatureInvariants(mol)\n","        fp = AllChem.GetMorganFingerprintAsBitVect(\n","            mol, radius, nBits=fpSize, invariants=invariants,\n","            useChirality=include_chirality\n","        )\n","        arr = np.zeros(fpSize, dtype=int)\n","        ConvertToNumpyArray(fp, arr)\n","        return arr\n","\n","    elif mode == \"RDK\":\n","        fpSize = kwargs.get(\"fpSize\", 2048)\n","        fp = RDKFingerprint(mol, fpSize=fpSize)\n","        arr = np.zeros(fpSize, dtype=int)\n","        ConvertToNumpyArray(fp, arr)\n","        return arr\n","\n","    elif mode == \"Descriptors\":\n","        desc_list = [desc[0] for desc in Descriptors._descList]\n","        calc = MoleculeDescriptors.MolecularDescriptorCalculator(desc_list)\n","        mol = Chem.AddHs(mol)\n","        descriptors = calc.CalcDescriptors(mol)\n","        return np.array(descriptors)\n","\n","    else:\n","        raise ValueError(f\"Unknown mode: {mode}\")\n","\n","smiles_list = data['SMILES'].tolist()"],"metadata":{"id":"Ag15vNNKKYka","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1757955288134,"user_tz":420,"elapsed":8413,"user":{"displayName":"Alexa Canchola","userId":"12486619005608823341"}},"outputId":"2c54650c-82f5-4fd5-a687-cc8ae5223ffb"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1400792472.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Descriptors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msmiles_to_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msmiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmiles_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Descriptors\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1400792472.py\u001b[0m in \u001b[0;36msmiles_to_feature\u001b[0;34m(smiles, mode, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mcalc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoleculeDescriptors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolecularDescriptorCalculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddHs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdescriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalcDescriptors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rdkit/ML/Descriptors/MoleculeDescriptors.py\u001b[0m in \u001b[0;36mCalcDescriptors\u001b[0;34m(self, mol, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDescriptorsMod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rdkit/Chem/EState/EState.py\u001b[0m in \u001b[0;36mMinEStateIndex\u001b[0;34m(mol, force)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mMinEStateIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEStateIndices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rdkit/Chem/EState/EState.py\u001b[0m in \u001b[0;36mEStateIndices\u001b[0;34m(mol, force)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eStateIndices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mtbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetPeriodicTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0mnAtoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetNumAtoms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mIs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnAtoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# ============================================================================\n","# Generate desired fingerprints/molecular descriptors from SMILES\n","# ============================================================================\n","# === CHOOSE FP MODE HERE  ===\n","# Choose mode: \"MACCS\", \"Morgan\", \"FCFP\", \"RDK\", or \"Descriptors\"\n","mode = \"Descriptors\"\n","\n","X = np.array([smiles_to_feature(smiles, mode=mode) for smiles in smiles_list])\n","\n","if mode == \"Descriptors\":\n","    desc_names = [\"RDKit_\" + desc[0] for desc in Descriptors._descList]\n","    X_df = pd.DataFrame(X, columns=desc_names)\n","    X_df = X_df.replace([np.inf, -np.inf], np.nan).fillna(0) # Replace inf and NaN, then scale to range of 0,1\n","    scaler = MinMaxScaler()\n","    X_scaled = scaler.fit_transform(X_df)\n","    X_df = pd.DataFrame(X_scaled, columns=X_df.columns)\n","else:\n","    X_df = pd.DataFrame(X, columns=[f'FP_{i}' for i in range(X.shape[1])])\n","\n","print(X_df)"],"metadata":{"id":"6UiNJkVBhsF3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Functions to Convert Dataframes to PyTorch Tensors\n","# ============================================================================\n","\n","def convert_to_X_tensors(X_train, X_test):\n","    X_train_np = X_train.to_numpy()\n","    X_test_np = X_test.to_numpy()\n","\n","    X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n","    X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n","\n","    return X_train_tensor, X_test_tensor\n","\n","def convert_to_Y_tensors(Y_train, Y_test):\n","    Y_train_np = Y_train.to_numpy()\n","    Y_train_np = np.where(pd.isnull(Y_train_np), np.nan, Y_train_np)\n","\n","    Y_test_np = Y_test.to_numpy()\n","    Y_test_np = np.where(pd.isnull(Y_test_np), np.nan, Y_test_np)\n","\n","    Y_train_tensor = torch.tensor(Y_train_np, dtype=torch.float32)\n","    Y_test_tensor = torch.tensor(Y_test_np, dtype=torch.float32)\n","\n","    print(Y_train_tensor.shape)\n","    print(Y_test_tensor.shape)\n","\n","    return Y_train_tensor, Y_test_tensor"],"metadata":{"id":"XZ0XWUOiO14Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Neural Network Definition and Cross-Validation Training Function\n","# ============================================================================\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self, input_size, shared_layer_sizes, output_layer_sizes, specific_layer_sizes):\n","        super(NeuralNetwork, self).__init__()\n","        self.shared_layers = nn.ModuleList()\n","        self.shared_layers.append(nn.Linear(input_size, shared_layer_sizes[0]))\n","        for i in range(len(shared_layer_sizes) - 1):\n","            self.shared_layers.append(nn.Linear(shared_layer_sizes[i], shared_layer_sizes[i + 1]))\n","        self.output_layers = nn.ModuleList()\n","        for output_size in output_layer_sizes:\n","            specific_layers = nn.Sequential(\n","                nn.Linear(shared_layer_sizes[-1], specific_layer_sizes[0]),\n","                nn.ReLU(),\n","                nn.Linear(specific_layer_sizes[0], output_size)\n","            )\n","            self.output_layers.append(specific_layers)\n","\n","    def forward(self, x):\n","        for layer in self.shared_layers:\n","            x = torch.relu(layer(x))\n","        outputs = [output_layer(x) for output_layer in self.output_layers]\n","        outputs = [torch.sigmoid(output_layer(x)) for output_layer in self.output_layers]\n","        return outputs\n","\n","# Modify the train_model function to integrate SMOTE\n","def train_model(X, Y, learning_rate, epochs, batch_size, shared_layer_sizes, output_layer_sizes, specific_layer_sizes, patience, l2_strength):\n","\n","    # Use KFold for multilabel tasks\n","    kf = KFold(n_splits=5)#, shuffle=True, random_state=42)\n","    best_model = None\n","    best_f1_score = 0\n","    best_balanced_acc = 0\n","    best_auc = 0\n","    best_mcc = 0\n","    outer_results = []\n","\n","    for train_index, val_index in kf.split(X, Y):  # Stratified split based on labels Y_filtered\n","        X_train, X_val = X[train_index], X[val_index]\n","        Y_train, Y_val = Y[train_index], Y[val_index]\n","\n","        model = NeuralNetwork(input_size=X.shape[1], shared_layer_sizes=shared_layer_sizes, output_layer_sizes=output_layer_sizes, specific_layer_sizes=specific_layer_sizes)\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","        epochs_without_improvement = 0\n","        for epoch in range(epochs):\n","            model.train()\n","            for i in range(0, len(X_train), batch_size):\n","                X_batch = X_train[i:i + batch_size]\n","                Y_batch = Y_train[i:i + batch_size]\n","                optimizer.zero_grad()\n","                outputs = model(X_batch)\n","\n","                losses = []\n","                for j, output in enumerate(outputs):\n","                    mask = ~torch.isnan(Y_batch[:, j])  # Create mask for valid labels\n","                    if mask.sum() > 0:\n","                        valid_output = output.squeeze(1)[mask]\n","                        valid_targets = Y_batch[:, j][mask]\n","                        loss = nn.BCELoss()(valid_output, valid_targets)\n","                        losses.append(loss)\n","\n","                if losses:  # Avoid zero loss if all values are NaN\n","                    total_loss = sum(losses)\n","\n","                    # L2 Regularization\n","                    l2_loss = sum(torch.norm(param, 2) for param in model.parameters())\n","                    total_loss += l2_strength * l2_loss  # Add L2 loss to the total loss\n","\n","                    total_loss.backward()\n","                    optimizer.step()\n","\n","            # Validation\n","            model.eval()\n","            with torch.no_grad():\n","                val_true_labels = Y_val.numpy()\n","                val_pred_probs_list = model(X_val)\n","                val_pred_list = [torch.where(output > 0.5, torch.tensor(1.0), torch.tensor(0.0)) for output in val_pred_probs_list]\n","\n","                val_pred_probs = torch.stack(val_pred_probs_list, dim=1)  # Stack all outputs\n","                val_pred = torch.stack(val_pred_list, dim=1)  # Stack all outputs\n","\n","                roc_auc_each_output = []\n","                balanced_acc_each_output = []\n","                mcc_each_output = []\n","                accuracy_each_output = []\n","                precision_each_output = []\n","                recall_each_output = []\n","                f1_score_each_output = []\n","\n","                for j in range(val_pred_probs.shape[1]):  # Iterate over each task (column)\n","\n","                    valid_mask = ~np.isnan(val_true_labels[:, j])  # Mask to filter out NaN values\n","                    valid_true_labels = val_true_labels[valid_mask, j]\n","                    val_pred_task = val_pred[valid_mask, j].numpy()\n","                    val_pred_probs_task = val_pred_probs[valid_mask, j].numpy()\n","\n","                    if valid_mask.sum() > 0:\n","\n","                        results = model_metrics(valid_true_labels, val_pred_task, val_pred_probs_task)\n","\n","                        # Calculate AUC if there are both positive and negative samples\n","                        if np.sum(valid_true_labels) > 0 and np.sum(valid_true_labels) < len(valid_true_labels):\n","                            roc_auc_each_output.append(results[0])\n","                        else:\n","                            roc_auc_each_output.append(0.5)  # Typically represents a random guess\n","\n","                        balanced_acc_each_output.append(results[1])\n","                        mcc_each_output.append(results[2])\n","                        accuracy_each_output.append(results[3])\n","                        precision_each_output.append(results[4])\n","                        recall_each_output.append(results[5])\n","                        f1_score_each_output.append(results[6])\n","\n","            # Calculate average metrics of the outputs\n","            '''\n","            roc_auc = harmonic_mean(roc_auc_each_output)\n","            balanced_acc = harmonic_mean(balanced_acc_each_output)\n","            mcc = harmonic_mean(mcc_each_output)\n","            accuracy = harmonic_mean(accuracy_each_output)\n","            precision = harmonic_mean(precision_each_output)\n","            recall = harmonic_mean(recall_each_output)\n","            f1_score = harmonic_mean(f1_score_each_output)\n","            #'''\n","\n","            '''\n","            roc_auc = np.mean(roc_auc_each_output)\n","            balanced_acc = np.mean(balanced_acc_each_output)\n","            mcc = np.mean(mcc_each_output)\n","            accuracy = np.mean(accuracy_each_output)\n","            precision = np.mean(precision_each_output)\n","            recall = np.mean(recall_each_output)\n","            f1_score = np.mean(f1_score_each_output)\n","            #'''\n","\n","            #'''\n","            weights = [0.4, 0.6]\n","            roc_auc = weighted_average(roc_auc_each_output, weights)\n","            balanced_acc = weighted_average(balanced_acc_each_output, weights)\n","            mcc = weighted_average(mcc_each_output, weights)\n","            accuracy = weighted_average(accuracy_each_output, weights)\n","            precision = weighted_average(precision_each_output, weights)\n","            recall = weighted_average(recall_each_output, weights)\n","            f1_score = weighted_average(f1_score_each_output, weights)\n","            #'''\n","\n","            # Append the outer results\n","            outer_results.append({\n","                \"outer_auc\": roc_auc,\n","                \"outer_balanced_accuracy\": balanced_acc,\n","                \"outer_mcc\": mcc,\n","                \"outer_accuracy\": accuracy,\n","                \"outer_precision\": precision,\n","                \"outer_recall\": recall,\n","                \"outer_f1\": f1_score,\n","            })\n","\n","            if f1_score >= best_f1_score:\n","                best_f1_score = f1_score\n","                best_balanced_acc = balanced_acc\n","                best_auc = roc_auc\n","                best_mcc = mcc\n","                best_model = model\n","                epochs_without_improvement = 0\n","            else:\n","                epochs_without_improvement += 1\n","\n","            # Check for early stopping after evaluating all tasks\n","            if epochs_without_improvement >= patience:\n","                break\n","\n","    # Calculate mean and standard deviation of the metrics over all folds\n","\n","    ave_auc, ave_f1, ave_balanced_acc, avg_ncv_mcc, std_ncv_mcc = calculate_cv_avg_std(outer_results)\n","\n","    if (std_ncv_mcc > avg_ncv_mcc) | (avg_ncv_mcc <= 0.0):\n","        return 0.00\n","    else:\n","        return ave_auc"],"metadata":{"id":"P7gupGd-SQKQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Prepare Target Variables (Y)\n","# ============================================================================\n","target_columns = ['Cancer','Genotoxicity']\n","Y1 = data[target_columns].copy()\n","\n","# Replace 'na' with NaN and convert to numeric\n","Y1 = Y1.replace('na', np.nan)\n","for col in target_columns:\n","    Y1[col] = pd.to_numeric(Y1[col])\n","\n","Y1_df = pd.DataFrame(Y1, columns=target_columns)\n","Y1 = Y1_df.values\n","\n","#Check df\n","print(Y1_df)\n","\n","X1_clean_df = X_df.loc[Y1_df.dropna(thresh=1).index] # thresh=3\n","Y1_clean_df = Y1_df.dropna(thresh=1)\n","print(X1_clean_df.shape)\n","print(Y1_clean_df.shape)"],"metadata":{"id":"AyqQXVh1X0vX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Split the Dataset into Training and Test Sets\n","# ============================================================================\n","X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1_clean_df, Y1_clean_df, test_size=0.2, random_state=42) # 80:20 split\n","\n","print('--------------------------------------------------------------')\n","print('Cancer')\n","CheckData(Y1_train, 'Cancer')\n","print('--------------------------------------------------------------')\n","print('Cancer')\n","CheckData(Y1_test, 'Cancer')\n","print('--------------------------------------------------------------')\n","CheckData(Y1_train, 'Genotoxicity')\n","print('--------------------------------------------------------------')\n","CheckData(Y1_test, 'Genotoxicity')"],"metadata":{"id":"KAMh8lniYEfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Apply Random Oversampling to Training Dataset to Reach Target Size\n","# ============================================================================\n","# Desired total number of training samples\n","target_size = 1268\n","\n","current_size = len(X1_train)\n","num_extra = target_size - current_size\n","\n","# Randomly sample (with replacement) from existing data\n","np.random.seed(42)\n","resample_indices = np.random.choice(current_size, size=num_extra, replace=True)\n","\n","X_extra = X1_train.iloc[resample_indices]\n","Y_extra = Y1_train.iloc[resample_indices]\n","\n","# Combine original and extra\n","X_combined = pd.concat([X1_train, X_extra], axis=0).reset_index(drop=True)\n","Y_combined = pd.concat([Y1_train, Y_extra], axis=0).reset_index(drop=True)\n","\n","# Shuffle both DataFrames in unison\n","shuffled_indices = np.random.permutation(target_size)\n","X_train_resampled = X_combined.iloc[shuffled_indices].reset_index(drop=True)\n","Y1_train_resampled = Y_combined.iloc[shuffled_indices].reset_index(drop=True)\n","\n","# Check new size\n","print(X_train_resampled.shape)  # Should be (1268, n_features)\n","print(Y1_train_resampled.shape) # Should be (1268, 3)"],"metadata":{"id":"jxOkuratY3X7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Convert Resampled Datasets to Pytorch Tensors\n","# ============================================================================\n","X1_train_tensor, X1_test_tensor = convert_to_X_tensors(X_train_resampled, X1_test)\n","Y1_train_tensor, Y1_test_tensor = convert_to_Y_tensors(Y1_train_resampled, Y1_test)"],"metadata":{"id":"4WVQk3hUZs2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Bayesian Hyperparameter Optimization Using gp_minimize (AUC Objective)\n","# ============================================================================\n","start_time = time.time() # (Optional): Record Elapse Time for Training\n","\n","# Objective function for gp_minimize\n","def objective(params):\n","    learning_rate, batch_size, epochs, shared_layer_size, specific_layer_size, l2_strength = params\n","\n","    best_auc = train_model(\n","        X1_train_tensor,\n","        Y1_train_tensor,\n","        learning_rate,\n","        epochs,\n","        batch_size,\n","        [shared_layer_size],\n","        [1, 1],\n","        [specific_layer_size, specific_layer_size],\n","        patience=20,\n","        l2_strength=l2_strength  # Pass L2 strength to the train_model\n","    )\n","    print('----------------------------------------------------------')\n","    print(f\"Best AUC: {best_auc:.3f}\")\n","    opt_criteria = float(best_auc) * (-1)\n","\n","    return opt_criteria  # Minimize negative AUC\n","\n","# Define the hyperparameter search space\n","search_space = [\n","    (1e-4, 1e-1),    # Learning rate\n","    (21, 37),         # Batch size\n","    (30, 80),        # Epochs\n","    (64, 256),      # Shared layer size\n","    (32, 125),       # Specific layer size\n","    (1e-4, 1e-2)     # L2 regularization strength\n","]\n","\n","class CallIndexPrinter:\n","    def __init__(self):\n","        self.index = 1\n","\n","    def __call__(self, res):\n","        print(f\"Call index {self.index} finished.\")\n","        self.index += 1\n","\n","print('Start training:')\n","call_index_printer = CallIndexPrinter()\n","\n","# Perform Bayesian optimization with the callback\n","results = gp_minimize(objective, search_space, n_calls=40, random_state=seed, callback=[call_index_printer])\n","\n","# Print the best parameters\n","best_params = results.x\n","best_auc = -results.fun\n","\n","print(\"Best hyperparameters: Learning Rate: {:.5f}, Batch Size: {}, Epochs: {}, Shared Layer Size: {}, Specific Layer Size: {}, L2 regularization strength: {}\".format(\n","    best_params[0], best_params[1], best_params[2], best_params[3], best_params[4], best_params[5]\n","))\n","print(\"Best AUC: {:.3f}\".format(best_auc))\n","\n","end_time = time.time()\n","print(f\"Elapsed Time: {end_time - start_time:.2f} seconds\") #(Optional): Print elapsed time"],"metadata":{"id":"Jdpy6Ah5Zv2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Final Training of Best Neural Network Model and Evaluation on Training Data\n","# ============================================================================\n","\n","start_time = time.time() # (Optional): Record Elapse Time for Training\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","best_model = NeuralNetwork(\n","    input_size=X1_train_tensor.shape[1],\n","    shared_layer_sizes=[best_params[3]],  # Two shared layers\n","    output_layer_sizes=[1,1],  # Output sizes\n","    specific_layer_sizes=[best_params[4], best_params[4]]  # Specific layer sizes\n",")\n","\n","optimizer = optim.Adam(best_model.parameters(), lr=best_params[0])\n","best_model.train()\n","\n","# Training loop\n","for epoch in range(best_params[2]):  # Use the best number of epochs\n","    for i in range(0, len(X1_train_tensor), best_params[1]):  # Use the best batch size\n","        X1_batch = X1_train_tensor[i:i + best_params[1]]\n","        Y1_batch = Y1_train_tensor[i:i + best_params[1]]\n","        optimizer.zero_grad()\n","        outputs = best_model(X1_batch)\n","\n","        losses = []\n","        for j, output in enumerate(outputs):\n","            mask = ~torch.isnan(Y1_batch[:, j])  # Create mask for valid labels\n","            if mask.sum() > 0:\n","                valid_output = output.squeeze(1)[mask]\n","                valid_targets = Y1_batch[:, j][mask]\n","                loss = nn.BCELoss()(valid_output, valid_targets)\n","                losses.append(loss)\n","\n","        if losses:\n","            total_loss = sum(losses)\n","\n","            # L2 Regularization\n","            l2_loss = sum(torch.norm(param, 2) for param in best_model.parameters())\n","            total_loss += best_params[5] * l2_loss  # Add L2 loss to the total loss\n","\n","            total_loss.backward()\n","            optimizer.step()\n","\n","\n","#Print Final Training Results\n","best_model.eval()\n","with torch.no_grad():\n","    train_outputs = best_model(X1_train_tensor)\n","    train_pred_probs = [output.squeeze().numpy() for output in train_outputs]\n","\n","task_names = ['Cancer', 'Genotoxicity']\n","num_tasks = len(task_names)\n","Y1_train_np = Y1_train_tensor.numpy()\n","\n","for task_idx in range(num_tasks):\n","    valid_mask = ~np.isnan(Y1_train_np[:, task_idx])\n","    y_true = Y1_train_np[valid_mask, task_idx]\n","    y_prob = train_pred_probs[task_idx][valid_mask]\n","\n","    # Calculate ROC AUC\n","    fpr, tpr, _ = roc_curve(y_true, y_prob, pos_label=1)\n","    roc_auc = auc(fpr, tpr)\n","\n","    # Calculate predicted labels with threshold 0.5\n","    y_pred = (y_prob > 0.5).astype(float)\n","\n","    # Compute metrics (replace with your model_metrics or sklearn functions)\n","    results = model_metrics(y_true, y_pred, y_prob)\n","\n","    print('---------------------------------------------')\n","    print(f'Training Results for {task_names[task_idx]}:')\n","    print(f'ROC AUC: {roc_auc:.3f}')\n","    print('Other metrics (Balanced Accuracy, MCC, Accuracy, Precision, Recall, F1):')\n","    print(\", \".join([f\"{res:.3f}\" for res in results]))\n","\n","end_time = time.time()\n","print(f\"Elapsed Time: {end_time - start_time:.2f} seconds\") # (Optional): Record Elapse Time for Training\n"],"metadata":{"id":"ujrNvZ1ZaVIf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Save trained model & hyperparameters\n","# ============================================================================\n","model_path = \"best_model_CarGen_multi-task.pth\"\n","torch.save({\n","    'model_state_dict': best_model.state_dict(),\n","    'params': best_params,\n","    'input_size': X1_train_tensor.shape[1],\n","}, model_path)\n","\n","# Save scaler (if applicable)\n","if mode == \"Descriptors\":\n","    scaler_path = \"descriptor_scaler.pkl\"\n","    joblib.dump(scaler, scaler_path)\n","\n","# Download files if using Google Colab\n","try:\n","    from google.colab import files\n","    files.download(model_path)\n","    if mode == \"Descriptors\":\n","        files.download(scaler_path)\n","except ImportError:\n","    print(f\"Files saved locally: {model_path} and (if used) {scaler_path}\")"],"metadata":{"id":"YTAecQajOEN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Test Set Evaluation Using Best Neural Network Model\n","# ============================================================================\n","\n","best_model.eval()  # Set the model to evaluation mode\n","with torch.no_grad():\n","    test_outputs = best_model(X1_test_tensor)\n","    test_pred_probs = torch.stack(test_outputs, dim=1)  # Stack all outputs\n","\n","task_names = ['Cancer','Genotoxicity']\n","num_tasks = len(task_names)\n","\n","# Create a single plot for all tasks' ROC curves\n","plt.figure(figsize=(5, 4))\n","accuracies = []\n","\n","# Loop through each task to plot the ROC curves on the same panel\n","for task_idx in range(num_tasks):\n","    Y1_test_np = Y1_test.to_numpy()\n","    valid_mask = ~np.isnan(Y1_test_np[:, task_idx])  # Mask for valid labels\n","    valid_true_labels = Y1_test_np[valid_mask, task_idx]\n","    test_pred_probs_task = test_pred_probs[valid_mask, task_idx].numpy()\n","\n","    # Calculate ROC curve\n","    fpr, tpr, _ = roc_curve(valid_true_labels, test_pred_probs_task, pos_label=1)\n","    roc_auc = auc(fpr, tpr)\n","    print('--------------------------------------------------------------------')\n","    print('AUC for ' + task_names[task_idx] + ': ' + str(roc_auc))\n","    print('FPR for ' + task_names[task_idx] + ': ' + str(fpr))\n","    print('TPR for ' + task_names[task_idx] + ': ' + str(tpr))\n","\n","    # Plot ROC curve for the current task\n","    plt.plot(fpr, tpr, lw=2, label=f'{task_names[task_idx]} (AUC = {roc_auc:.2f})')\n","\n","    predicted_labels = np.array([1.0 if output > 0.5 else 0.0 for output in test_pred_probs_task])\n","    test_results = model_metrics(valid_true_labels, predicted_labels, test_pred_probs_task)\n","\n","    print('--------------------------------------------------------------------')\n","    print(\"Overall Results for Test Set:\")\n","    print(\"ROC-AUC, Balanced Accuracy, MCC, Accuracy, Precision, Recall, F1:\")\n","    print(\", \".join([f\"{result:.3f}\" for result in test_results]))\n","\n","plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Random classifier line\n","\n","plt.xticks(np.arange(0, 1.1, 0.2), fontsize=12)\n","plt.yticks(np.arange(0, 1.1, 0.2), fontsize=12)\n","plt.xlim([-0.01, 1.01])\n","plt.ylim([-0.01, 1.01])\n","plt.xlabel('False Positive Rate', fontsize=12)\n","plt.ylabel('True Positive Rate', fontsize=12)\n","plt.title('', fontsize=12)\n","plt.grid(True)\n","plt.legend(loc='lower right', fontsize=8.6)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TULp9-YsbJI2","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Generate Applicability Domain Plots\n","# ============================================================================\n","def dropout_uncertainty(model, X, task_idx, num_samples=100):\n","    model.train()  # Set the model in train mode to keep dropout active\n","    predictions = []\n","    for _ in range(num_samples):\n","        output = model(X)[task_idx]\n","        predictions.append(output.detach().cpu().numpy())\n","\n","    predictions = np.array(predictions)  # Shape: (num_samples, n_samples)\n","    mean_predictions = predictions.mean(axis=0)\n","    std_predictions = predictions.std(axis=0)\n","    return mean_predictions.squeeze(), std_predictions.squeeze()\n","\n","n_samples = 50\n","epsilon = 1.5e-2\n","\n","task_names = ['Cancer', 'Genotoxicity']\n","\n","# Containers for predictions per task\n","mean_predictions_train = []\n","std_predictions_train = []\n","mean_predictions_test = []\n","std_predictions_test = []\n","\n","for i in range(len(task_names)):\n","    mean_train, std_train = dropout_uncertainty(best_model, X1_train_tensor, task_idx=i, num_samples=n_samples)\n","    mean_test, std_test = dropout_uncertainty(best_model, X1_test_tensor, task_idx=i, num_samples=n_samples)\n","\n","    mean_predictions_train.append(mean_train)\n","    std_predictions_train.append(std_train)\n","    mean_predictions_test.append(mean_test)\n","    std_predictions_test.append(std_test)\n","\n","all_std_train = np.concatenate(std_predictions_train)\n","uncertainty_threshold = np.percentile(all_std_train, 95)  # 95th percentile threshold\n","\n","X_train_np = X_train_resampled.values\n","X_test_np = X1_test.values\n","\n","# Robust scaling\n","scaler = RobustScaler()\n","X_train_scaled = scaler.fit_transform(X_train_np)\n","X_test_scaled = scaler.transform(X_test_np)\n","\n","\n","# PCA\n","pca = PCA(n_components=10)  # Choose appropriate number of components\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","# Compute train centroid\n","centroid = X_train_pca.mean(axis=0)\n","\n","# Compute distances\n","distances_train = np.linalg.norm(X_train_pca - centroid, axis=1)\n","distances_test = np.linalg.norm(X_test_pca - centroid, axis=1)\n","\n","# Close previous figures\n","plt.close('all')\n","\n","fig, ax = plt.subplots(figsize=(8, 7))\n","\n","colors = {'Train': 'blue', 'Test': 'orange'}\n","markers = {'Cancer': 'o', 'Genotoxicity': 'v'}\n","\n","for i, task_names in enumerate(task_names):\n","    y_true_train = Y1_train_tensor[:, i].numpy()\n","    mask_train = ~np.isnan(y_true_train)\n","    y_true_train = y_true_train[mask_train]\n","    y_predictions_train = np.clip(mean_predictions_train[i][mask_train], epsilon, 1 - epsilon)\n","\n","    residuals_train = y_true_train - y_predictions_train\n","    pearson_train = residuals_train / np.sqrt(y_predictions_train * (1 - y_predictions_train))\n","\n","    y_true_test = Y1_test_tensor[:, i].numpy()\n","    mask_test = ~np.isnan(y_true_test)\n","    y_true_test = y_true_test[mask_test]\n","    y_predictions_test = np.clip(mean_predictions_test[i][mask_test], epsilon, 1 - epsilon)\n","\n","    residuals_test = y_true_test - y_predictions_test\n","    pearson_test = residuals_test / np.sqrt(y_predictions_test * (1 - y_predictions_test))\n","\n","    ax.scatter(distances_train[mask_train], pearson_train, alpha=0.5,\n","               color=colors['Train'], marker=markers[task_names],\n","               label=f'{task_names} (Train)')\n","\n","    ax.scatter(distances_test[mask_test], pearson_test, alpha=0.7,\n","               color=colors['Test'], marker=markers[task_names],\n","               label=f'{task_names} (Test)')\n","\n","\n","# Threshold lines\n","distance_threshold = distances_train.mean() + 3 * distances_train.std()\n","ax.axvline(distance_threshold, color='green', linestyle='--', linewidth=1)\n","ax.axhline(3, color='red', linestyle='--', linewidth=1)\n","ax.axhline(-3, color='red', linestyle='--', linewidth=1)\n","\n","label_fontsize = 16\n","title_fontsize = 18\n","tick_fontsize = 14\n","legend_fontsize = 12\n","\n","ax.set_xlabel('PCA Distance to Training Centroid', fontsize=label_fontsize)\n","ax.set_ylabel(\"Pearson's Residuals\", fontsize=label_fontsize)\n","ax.set_title(\"Applicability Domain via PCA Distance\", fontsize=title_fontsize)\n","\n","ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n","\n","ax.legend(fontsize=legend_fontsize)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"rOMFVMSDbwe-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# (OPTIONAL): Calculate % of Training/Test Within AD for Each Task\n","# ============================================================================\n","print('--------------------------------------------------------------------')\n","print('Applicability Domain Coverage')\n","print('--------------------------------------------------------------------')\n","\n","for i, task_names in enumerate(task_names):\n","    # Masks for valid samples\n","    y_true_train = Y1_train_tensor[:, i].numpy()\n","    mask_train = ~np.isnan(y_true_train)\n","\n","    y_true_test = Y1_test_tensor[:, i].numpy()\n","    mask_test = ~np.isnan(y_true_test)\n","\n","    # Compute distances\n","    distances_train_task = distances_train[mask_train]\n","    distances_test_task = distances_test[mask_test]\n","\n","    # Get predicted probabilities\n","    y_predictions_train = np.clip(mean_predictions_train[i][mask_train], epsilon, 1 - epsilon)\n","    y_predictions_test = np.clip(mean_predictions_test[i][mask_test], epsilon, 1 - epsilon)\n","\n","    # Residuals\n","    residuals_train = y_true_train[mask_train] - y_predictions_train\n","    pearson_train = residuals_train / np.sqrt(y_predictions_train * (1 - y_predictions_train))\n","\n","    residuals_test = y_true_test[mask_test] - y_predictions_test\n","    pearson_test = residuals_test / np.sqrt(y_predictions_test * (1 - y_predictions_test))\n","\n","    # Conditions for within AD\n","    within_ad_train = (distances_train_task <= distance_threshold) & (np.abs(pearson_train) <= 3)\n","    within_ad_test = (distances_test_task <= distance_threshold) & (np.abs(pearson_test) <= 3)\n","\n","    # Calculate percentages\n","    percent_within_ad_train = 100.0 * np.sum(within_ad_train) / len(distances_train_task)\n","    percent_within_ad_test = 100.0 * np.sum(within_ad_test) / len(distances_test_task)\n","\n","    print(f\"Task: {task_names}\")\n","    print(f\"  Train samples within AD: {percent_within_ad_train:.2f}%\")\n","    print(f\"  Test samples within AD:  {percent_within_ad_test:.2f}%\\n\")\n","\n"],"metadata":{"id":"IdABlCsch-I1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Visualize Confusion Matrices for Each Task\n","# ===============================================================================\n","def plot_confusion_matrix(Y_test, predicted_labels, task_names='Task'):\n","    cm = confusion_matrix(Y_test, predicted_labels)\n","\n","    print('--------------------------------------------------------------------')\n","    print(f\"Confusion Matrix for {task_names}:\")\n","    print(cm)\n","\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',\n","                xticklabels=['Negative', 'Positive'],\n","                yticklabels=['Negative', 'Positive'],\n","                annot_kws={\"size\": 16})\n","\n","    plt.title(f\"Confusion Matrix - {task_names}\")\n","    plt.xlabel('Predicted',fontsize=20)\n","    plt.xticks(fontsize=18)\n","    plt.ylabel('True',fontsize=20)\n","    plt.yticks(fontsize=18)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","threshold = 0.5\n","task_names = ['Cancer', 'Genotoxicity']\n","\n","for i, task_names in enumerate(task_names):\n","    # True labels for test set\n","    y_true_test = Y1_test_tensor[:, i].numpy()\n","    mask_test = ~np.isnan(y_true_test)\n","    y_true_test_clean = y_true_test[mask_test].astype(int)\n","\n","    # Predicted probabilities\n","    y_pred_probs = mean_predictions_test[i][mask_test]\n","    # Convert to binary labels\n","    y_pred_labels = (y_pred_probs >= threshold).astype(int)\n","\n","    # Plot confusion matrix\n","    plot_confusion_matrix(y_true_test_clean, y_pred_labels, task_names=task_names)\n"],"metadata":{"id":"64cmdR1EiCfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Feature Importance Analysis: SHapley Additive exPlanation (SHAP) Plot Generation\n","# ============================================================================\n","num_features_to_show = 10\n","\n","best_model.eval()\n","X_test_np = X1_test_tensor.numpy()\n","Y_test_np = Y1_test_tensor.numpy()\n","\n","feature_names = X_train_resampled.columns.tolist()\n","\n","# Use a small sample to avoid heavy computation\n","X_sample = X_test_np[:500]\n","X_sample_tensor = torch.tensor(X_sample, dtype=torch.float32)\n","\n","# Define a prediction function compatible with SHAP\n","def model_predict(x_numpy):\n","    x_tensor = torch.tensor(x_numpy, dtype=torch.float32)\n","    with torch.no_grad():\n","        outputs = best_model(x_tensor)\n","        return [out.squeeze().numpy() for out in outputs]\n","\n","explainer_list = []\n","shap_values_list = []\n","\n","# Set global font size\n","plt.rcParams.update({'font.size': 20})\n","\n","for task_idx, task_name in enumerate(task_names):\n","    def single_task_predict(x_numpy):\n","        probs = model_predict(x_numpy)\n","        return probs[task_idx]\n","\n","    explainer = shap.Explainer(single_task_predict, X_sample, max_evals=2 * X_sample.shape[1] + 1)\n","    shap_values = explainer(X_sample)\n","\n","    explainer_list.append(explainer)\n","    shap_values_list.append(shap_values)\n","\n","    # Beeswarm plot (Local SHAP)\n","    fig_height = 0.6 * num_features_to_show + 1.5\n","    plt.figure(figsize=(20, fig_height))\n","    shap.summary_plot(\n","        shap_values.values,\n","        X_sample,\n","        feature_names=feature_names,\n","        max_display=num_features_to_show,\n","        show=False,\n","    )\n","    plt.title(f\"SHAP Beeswarm for {task_name}\", fontsize=16)\n","    ax = plt.gca()\n","    ax.title.set_fontsize(16)\n","    ax.tick_params(axis='both', labelsize=20)\n","    plt.show()\n","\n","    # Bar plot (Global SHAP)\n","    fig_height = 0.6 * num_features_to_show + 1.5\n","    plt.figure(figsize=(20, fig_height))\n","    shap.summary_plot(\n","        shap_values.values,\n","        X_sample,\n","        feature_names=feature_names,\n","        plot_type=\"bar\",\n","        max_display=num_features_to_show,\n","        show=False,\n","    )\n","    ax = plt.gca()\n","    for patch in ax.patches:\n","        patch.set_facecolor(\"#888888\")\n","\n","    plt.title(f\"SHAP Mean |Value| Bar Plot for {task_name}\", fontsize=16)\n","    ax.title.set_fontsize(16)\n","    ax.tick_params(axis='both', labelsize=20)\n","    plt.show()\n"],"metadata":{"id":"XXsfMX0tiUpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# (OPTIONAL): Print Important Features and as CSV file\n","# ===============================================================================\n","num_features_to_show = 20\n","\n","for task_idx, task_name in enumerate(task_names):\n","    # Get shap values for this task\n","    shap_values_task = shap_values_list[task_idx].values\n","\n","    # Compute mean absolute SHAP values per feature\n","    mean_abs_shap = np.abs(shap_values_task).mean(axis=0)\n","\n","    # Get feature importance ranking\n","    feature_importance_df = pd.DataFrame({\n","        'feature': feature_names,\n","        'mean_abs_shap': mean_abs_shap\n","    })\n","\n","    # Sort by importance\n","    feature_importance_df = feature_importance_df.sort_values(by='mean_abs_shap', ascending=False)\n","\n","    # Take top N features\n","    top_features_df = feature_importance_df.head(num_features_to_show)[['feature']]\n","\n","    # Save to CSV\n","    csv_filename = f\"top_features_{task_name}.csv\"\n","    top_features_df.to_csv(csv_filename, index=False)\n","\n","    print(f\"Saved top features for task '{task_name}' to: {csv_filename}\")"],"metadata":{"id":"myANNyxOiXp9"},"execution_count":null,"outputs":[]}]}